---
title: "Fit species distributions models"
author: "UK Centre for Ecology and Hydrology - DECIDE"
output: html_document
params:
  n_folds: 10
  species_name: pieris_brassicae
  model_to_run: glm
  plot_diagnostics: FALSE
  run_test: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#set working directory to the project folder, not the R/scripts folder which it is by default because it's an Rmarkdown doc 
knitr::opts_knit$set(root.dir = '../../')
```

```{r,echo = F}
library(htmltools)
h1(params$species_name)
```

### Load packages

```{r packages}
library(dplyr) # for data manipulation and %>%
library(raster) # for working with raster data
library(terra) # alternative to working with raster data that might be faster, used sporadically
library(randomForest) #for fitting random forest models
library(mgcv) # for fitting GAMs
library(dismo) # for fitting MAXENT models
library(ggplot2) # for plots
```

### Parameters

To be set up as parametrised markdown

```{r parameters}
n_folds <- params$n_folds # how many folds for the cross validation
n_folds
species_name <- params$species_name # which species to model (scientific name, lowercase, underscore)
species_name
model_to_run <- params$model_to_run #which model to run: "glm","gam","rf","maxent"
model_to_run
plot_diagnostics <- params$plot_diagnostics #whether to produce diagnostic plots
plot_diagnostics
run_test <- params$run_test # whether this is a test run on smaller data
run_test
```

```{r,echo = F}
#for interactive testing purposes
if(F){
  n_folds <- 10
  species_name <- "pieris_brassicae"
  model_to_run <- "glm"
  model_to_run <- "gam"
  model_to_run <- "rf"
  model_to_run <- "maxent"
  plot_diagnostics <- F
  run_test <- T
}
```

### Load data

```{r load_data}
#records and pseudoabsences
pas <- readRDS("data/derived_data/species/pas.RDS")

#species list
species_list <- readRDS("data/derived_data/species/species_list.RDS")

#environmental data
env_dat <- raster::stack("data/derived_data/environmental/envdata_fixedcoasts_nocorrs_100m_GB.gri")
#env_dat <- terra::rast(env_dat) # alternative approach not used


if(run_test){
  #crop the raster for testing
  smaller_extent <- extent(c(440000, 500000, 400000, 500000))
  env_dat <- crop(x = env_dat, y = smaller_extent)
}
```

### Preparing data

Combine the pres/pseudoabsence with the environmental data to make one data frame that we'll be fitting models to.

This is where the number of pseudoabsenses relative to presences needs to be set

```{r prepare_data}
prepare_data <- function(species_name, pas, env_data){
  
  

  #get the presence/absence for the relvant species
  pas_species <- pas[[species_name]]
  
  #if testing only get 20k of each from yorkshire(ish)
  if(run_test){
    pas_species$Presence <- pas_species$Presence %>% filter(lat < 500000,lat>400000,lon>440000, lon <500000) %>% as.data.frame()
    pas_species$pseudoAbsence <- pas_species$pseudoAbsence %>% filter(lat < 500000,lat>400000,lon>440000, lon <500000) %>% as.data.frame()
    
    #for testing purposes subset to 20000 rows
    if (nrow(pas_species$Presence) > 20000){
      pas_species$Presence <- pas_species$Presence[1:20000,] %>% as.data.frame()
    }
    if (nrow(pas_species$pseudoAbsence) > 20000){
      pas_species$pseudoAbsence <- pas_species$pseudoAbsence[1:20000,] %>% as.data.frame()
    }
    
  }
  
  

  #extract environmental variables
  env_data <- terra::rast(env_data)
  pres <- data.frame(val = 1,terra::extract(x = env_data, y = pas_species$Presence[,c("lon","lat")]), pas_species$Presence[,c("lon","lat")])
  ab <- data.frame(val = 0, terra::extract(x = env_data, y = pas_species$pseudoAbsence[,c("lon","lat")]), pas_species$pseudoAbsence[,c("lon","lat")])
  
  #remove NAs
  pres <- na.omit(pres)
  ab <- na.omit(ab)
  
  # make into one data frame
  pres_and_ab <- bind_rows(pres,ab)
  pres_and_ab
}

prepared_data <- prepare_data(species_name,pas,env_dat)

prepared_data %>% group_by(val) %>% summarise(n = n())
```

Next, we define the folds in the data.

These folds are used in the bootstrapping and model assessment. By default we're using 10 folds so we'll fit 10 models which will train a unique set of 9 of those folds, and then will be assessed using the remaining fold. Folds are represented by a new column in the data which goes from `1:n_folds`

```{r fold_data}

fold_data <- function(prepared_data,n_folds=10){
  prepared_data$fold[prepared_data$val==1] <- prepared_data %>% filter(val == 1) %>% kfold(n_folds)
  prepared_data$fold[prepared_data$val==0] <- prepared_data %>% filter(val == 0) %>% kfold(n_folds)
  prepared_data
}

head(fold_data(prepared_data))

folded_data <- fold_data(prepared_data,n_folds)
```

Define a function that can determine weightings from data. If there are more presences than psuedoabsesnse then presences can be weighted down. This function determines those weights which can then be used as an argument in the model fitting function. If unbalanced number of records vs pseudo-absences the  provide some weighting information. Used for GLM, RF and GAM.

```{r define_weights_function}
determine_weights <- function(folded_data,fold_i){
  n_records <- folded_data %>% filter(val == 1,fold != fold_i) %>% nrow()
  n_abs <- folded_data %>% filter(val == 0,fold != fold_i) %>% nrow()
    
  prop <- n_records / n_abs
  
  #if unbalanced
  if (n_records != n_abs){
    weights <- c(rep(1, n_records), rep(prop, n_abs))
  } else {
    weights <- NULL
  }
  
  weights
}

# if(run_test){
#   prepared_data %>% fold_data %>% determine_weights(1)
# }

```

### Fitting models

This is where we define a function for each model to fit. Each function takes the folded data and the fold to use for assessing fit (the remaining data is used for training). The maxent model also has an argument for the environmental data `env_data`. The functions return the model object, the result of running `dismo::evaluate()` and the model summary result from `summary()`.

We currently fit four models.

Models:

 - GLM (`base`?)
 - GAM (`mgcv`)
 - Random forest (`randomForest`)
 - Maxent (`dismo`)

#### Maxent

Maxent models work best with equal number of presences and absences, so should match the number of presences and absences

```{r fit_maxent_function}
fit_maxent <- function(folded_data,fold_i){
  options(java.parameters = "-Xmx3g") #https://stackoverflow.com/questions/5374455/what-does-java-option-xmx-stand-for
  
  #fit the model
  mod <- maxent(x = folded_data %>% filter(fold != fold_i) %>% dplyr::select(-ID,-lat,-lon,-val,-fold),
                p = folded_data %>% filter(fold != fold_i) %>% dplyr::select(val)
                )
  
  #evaludate the model
  test_data <- folded_data %>% filter(fold == fold_i) %>% dplyr::select(-lat,-lon,-fold,-ID)
  evaluation <- dismo::evaluate(p = test_data %>% filter(val == 1), 
                            a = test_data %>% filter(val == 0),
                            mod, tr = seq(0, 1, length.out = 200))
  
  #return the model + evaludation
  return(list(model = mod,evaluation = evaluation,summary = summary(mod)))
}

# if(model_to_run == "maxent" & run_test){
#   #test the function
#   test_mod1 <- fit_maxent(folded_data,1,env_dat)
#   class(test_mod1$model) #determine the class:
#   # [1] "MaxEnt"
#   # attr(,"package")
#   # [1] "dismo"
# }

```

#### GLM

Fit a GLM using `glm()`

```{r fit_glm_function}
#from https://win-vector.com/2014/05/30/trimming-the-fat-from-glm-models-in-r/
clean_model = function(cm) {
  cm$y = c()
  cm$model = c()
  
  cm$residuals = c()
  cm$fitted.values = c()
  cm$effects = c()
  cm$qr$qr = c()  
  cm$linear.predictors = c()
  cm$weights = c()
  cm$prior.weights = c()
  cm$data = c()
  cm
}

fit_glm <- function(folded_data,fold_i){
  
  weights <- determine_weights(folded_data,fold_i)
  train_data <- folded_data %>% filter(fold != fold_i) %>% dplyr::select(-lat,-lon,-fold,-ID)
  test_data <- folded_data %>% filter(fold == fold_i) %>% dplyr::select(-lat,-lon,-fold,-ID)
  
  mod <- glm(val ~ ., data = train_data, 
                   family = binomial(link = "logit"),
                   weights = weights,
             model = F)
  
  evaluation <- dismo::evaluate(p = test_data %>% filter(val == 1), 
                                  a = test_data %>% filter(val == 0), 
                                  mod, tr = seq(0, 1, length.out = 200))
  mod_summary <- summary(mod)
  
  #currently not cleaning the model
  #mod <- clean_model(mod)
  
  #return the model + evaludation
  return(list(model = mod,evaluation = evaluation,summary = mod_summary))
}

# if(model_to_run == "glm" & run_test){
#   #test the function
#   test_mod2 <- fit_glm(folded_data,1)
#   class(test_mod2$model) #class:
#   # [1] "glm" "lm"
# }

```


#### Random forest

Fit a random forest with `randomForest` from package `randomForest`

```{r fit_rf_function}

fit_rf <- function(folded_data,fold_i){
  weights <- determine_weights(folded_data,fold_i)
  if(!is.null(weights)){
    weights <- list(first(weights),last(weights))
  }
  
  train_data <- folded_data %>% filter(fold != fold_i) %>% dplyr::select(-lat,-lon,-fold,-ID)
  test_data <- folded_data %>% filter(fold == fold_i) %>% dplyr::select(-lat,-lon,-fold,-ID)
  
  mod <- randomForest(x = train_data %>% dplyr::select(-val), 
                      y = train_data %>% dplyr::pull(val) %>% as.factor(), 
                      importance = T, 
                      norm.votes = TRUE)
  
  rf.pred <- predict(mod, type = "prob", newdata = test_data)[,2]
  evaluation <- dismo::evaluate(p = rf.pred[test_data$val == 1], 
                                a = rf.pred[test_data$val == 0], 
                                tr = seq(0, 1, length.out = 200))
  
  #return the model + evaluation
  return(list(model = mod,evaluation = evaluation,summary = summary(mod)))
}

# if(model_to_run == "rf" & run_test){
#   #test the function
#   test_mod3 <- fit_rf(folded_data,1)
#   class(test_mod3$model)
#   # [1] "randomForest"
# }
```

#### GAM

Fit a GAM using `gam` from package `mgcv`

```{r fit_gam_function}

fit_gam <- function(folded_data,fold_i,knots_gam = -1){
  train_data <- folded_data %>% filter(fold != fold_i) %>% dplyr::select(-lat,-lon,-fold,-ID)
  test_data <- folded_data %>% filter(fold == fold_i) %>% dplyr::select(-lat,-lon,-fold,-ID)
  weights <- determine_weights(folded_data,fold_i)
  
  
  ## create formula for gam
  l <- sapply(folded_data %>% filter(fold != fold_i) %>% dplyr::select(-lon,-lat,-fold,-ID), unique)
  ks <- data.frame(variable = rownames(data.frame(k = round(sapply(l, length))[-1])),k = data.frame(k = round(sapply(l, length))[-1]))
  rownames(ks) <- 1:nrow(ks)
  
  # drop variables according to number of knots asked for
  # -1 is basically 9 knots
  if(knots_gam == -1) {
    v_keep <- ks[ks$k > 11,]
    print(paste("variable dropped =", ks$variable[!ks$variable %in% v_keep$variable]))
  }
  
  # any others just keep the variables with over the number of knots
  if(knots_gam > 0) {
    v_keep <- ks[ks$k > (knots_gam+3),]
    print(paste("variable dropped =", ks$variable[!ks$variable %in% v_keep$variable]))
  }
  
  #create the formula
  form <- as.formula(paste0("val ~ s(", paste(v_keep$variable,
                                            ", k = ", knots_gam) %>%
                            paste0(collapse = ") + s("), ")"))
  
  #fit the model
  mod <- gam(formula = form, data = train_data, 
           family = binomial(link = 'logit'), 
           select = TRUE, method = 'REML', gamma = 1.4,
           weights = weights)
  
  
  evaluation <- dismo::evaluate(p = test_data %>% filter(val == 1), 
                                  a = test_data %>% filter(val == 0), 
                                  mod, tr = seq(0, 1, length.out = 200))
  
  #return the model + evaludation
  return(list(model = mod,evaluation = evaluation,summary = summary(mod)))
}

# if(model_to_run == "gam" & run_test){
#   #test the function
#   test_mod4 <- fit_gam(folded_data,1)
#   class(test_mod4$model)
#   # [1] "gam" "glm" "lm"
# }


```


### Make predictions

This is where we apply the fitted models to predict across the entire environmental raster. We do this across each of the 10 bootstrapped models and get a mean prediction value. We also calculate the standard deviation of the 10 predictions which is part of the DECIDE score.


First define a function for making the predictions. It takes a list of the same type of model but fitted from each bootstrap of the data and the environmental raster `env_data`.

```{r define_prediction_function}
#model list is a list of 10 models fitted from each bootstrap
get_predictions <- function(model_list,env_data) {
  #get the model class
  model_class <- class(model_list[[1]]$model)
  
  # number of bootstraps that were run
  k = length(model_list)
  
  # choose the type and index for predict function
  if("glm" %in% model_class){ #"glm" is in the model class for both gams and glms
    type <- "response"
    index <- NULL
  } else if ("randomForest" %in% model_class) {
    type <- "prob"
    index <- 2
  } else if ("MaxEnt" %in% model_class){
    type <- NULL
    index <- NULL
  }
  
  ## bootstrapped models
  
  # predict from each of the bootstrapped models and stack them together
  boots_out <- raster::stack(lapply(model_list, FUN = function(x) {predict(env_data, x$model, type=type, index=index)} ))
  
  ## quantiles
  mean_preds <- calc(boots_out, fun = mean, na.rm = T) # the mean
  rnge <- calc(boots_out, fun = function(x) {sd(x, na.rm = TRUE)}) # get the standard deviation
  # rnge <- quant_preds[[2]]-quant_preds[[1]] # get the range of max - min
    

  return(list(mean_predictions = mean_preds,
              quant_range = rnge))
}

#test
if(F){
  test <- get_predictions(list(test_mod1,test_mod1,test_mod1,test_mod1),env_dat)
}

```

Until this point we haven't run any models, we have just defined functions for running them. This is where we fit the models and run predictions. We use `lapply` to run the model on each fold of the data at the same time.

```{r fit_models_and_predict}

folds_list <- as.list(1:n_folds)

#run the models on the folded data
if(model_to_run == "glm"){
  
  mods <- lapply(folds_list,FUN = function(x){
    fit_glm(folded_data,x)
    })
  
} else if(model_to_run == "gam"){
  
  mods <- lapply(folds_list,FUN = function(x){
    fit_gam(folded_data,x)
    })
  
} else if(model_to_run == "rf"){
  
  mods <- lapply(folds_list,FUN = function(x){
    fit_rf(folded_data,x)
    })
  
} else if(model_to_run == "maxent"){
  
  mods <- lapply(folds_list,FUN = function(x){
    fit_maxent(folded_data,x)
    })
  
}

#get predictions
predictions <- get_predictions(mods,env_dat)

#checking model size
object.size(mods) %>% format(units = "Mb")
object.size(predictions) %>% format(units = "Mb")

```

Model outputs for the first model

```{r print_model_outputs}
#the model
#mods[[1]]$mod

#the evaluation by dismo
mods[[1]]$evaluation

#the model summary
mods[[1]]$summary

#model AUCs
sapply(mods, function(x) {slot(x$evaluation, "auc")})
```

### Save outputs

Now that we've run the models we need to save the outputs.

The outputs are saved in their corresponding folder in `data/derived_data/model_outputs_by_species` where there are 4 folders for each model type: `gam`, `glm`, `maxent`, `rf`. There is also an `ensemble` folder but that is filled in the next stage.

For each model run for each species we end up with 4 files

 * `mean_predictions_[SPECIES].grd/gri` is the mean predictions
 * `bootstrapped_sd_[SPECIES].grd/gri` is the standard deviation between each bootstrapped model prediction
 * `mean_AUC_[SPECIES].rds` is the mean AUC for that model/species combo stored as a single numeric value
 * `models_[SPECIES].rds` contains one of the models, the AUCs from each model, the mean AUC across all models (again) and the summaries for all models.
 
```{r save_outputs}

#save rasters
#write prediction
raster::writeRaster(predictions$mean_predictions,filename = paste0("data/derived_data/model_outputs_by_species/",model_to_run,"/mean_prediction_",species_name,".grd"),overwrite=T)

#write SDM uncertainty
raster::writeRaster(predictions$quant_range,filename = paste0("data/derived_data/model_outputs_by_species/",model_to_run,"/bootstrapped_sd_",species_name,".grd"),overwrite=T)

#save model info
#calculate mean AUC across all models, and generate summaries
AUCs <- sapply(mods, function(x) {slot(x$evaluation, "auc")})
mean_AUC <- AUCs %>% mean()
summaries <- lapply(mods,FUN = function(x){x$summary})

#remove the models for all but the first bootstrap
#but retain evaluations and summaries
for (i in 2:n_folds){
  mods[[i]]$model <- NULL
}

#check model size
object.size(mods) %>% format(units = "Mb")
object.size(summaries) %>% format(units = "Mb")

#create one object for saving
output <- 
  list(mods,
       AUCs = AUCs,
       mean_AUC = mean_AUC,
       summaries = summaries)

#check size of this object
object.size(output) %>% format(units = "Mb") # all models

#save output
saveRDS(output,file = paste0("data/derived_data/model_outputs_by_species/",model_to_run,"/models_",species_name,".rds"))

#also write AUCs elsewhere so that in order to determine which models to include in the ensemble we don't have to load in the full model object.
saveRDS(mean_AUC, file = paste0("data/derived_data/model_outputs_by_species/",model_to_run,"/mean_AUC_",species_name,".rds"))


```


### Diagnostic plots

```{r model_disgnostics1}
if(plot_diagnostics){
  plot(mods[[1]]$model)
}
```

```{r model_diagnostics2}

#mean prediction versus sd
plot(x = values(predictions$mean_predictions),y = values(predictions$quant_range))

#compare predicted vs actual (and pseudo)
test <- data.frame(folded_data[,c("lon","lat","val")],prediction =terra::extract(x = predictions$mean_predictions, y = folded_data[,c("lon","lat")])) %>% na.omit(test)

test %>%
  ggplot(aes(x = val, y = prediction))+
  geom_point()

#turn raster into a data frame for ggplotting
test_spdf <- as(predictions$mean_predictions, "SpatialPixelsDataFrame")
test_df <- as.data.frame(test_spdf)
colnames(test_df) <- c("value", "x", "y")

#plot raster and points
test %>%
  filter(val == 1) %>%
  ggplot(aes(x = lon,y = lat))+
  geom_tile(data=test_df, aes(x=x, y=y, fill=value), alpha=1)+
  scale_fill_gradient(low = "aliceblue",high = "chartreuse3")+
  geom_point(alpha = 1,shape = 16)+
  theme_classic()+
  labs(fill = "Probability of presence",x = "Eastings",y = "Northings")+
  coord_fixed()
  

#turn raster into a data frame for ggplotting
test_spdf <- as(predictions$quant_range, "SpatialPixelsDataFrame")
test_df <- as.data.frame(test_spdf)
colnames(test_df) <- c("value", "x", "y")

#plot raster and points
test %>% 
  filter(val == 1) %>%
  ggplot(aes(x = lon,y = lat))+
  geom_tile(data=test_df, aes(x=x, y=y, fill=value, alpha=1))+
  scale_fill_gradient(low = "aliceblue",high = "coral1")+
  geom_point(alpha = 1,shape = 16)+
  theme_classic()+
  labs(fill = "Model Uncertainty",x = "Eastings",y = "Northings")+
  coord_fixed()


```





